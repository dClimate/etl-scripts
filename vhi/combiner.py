
from kerchunk import combine
import abc
import time
import json
import re

import orjson
import xarray

import pathlib
import datetime

from dc_etl import filespec


from dc_etl.filespec import FileSpec
from dc_etl.combine import Combiner

HERE = filespec.file(pathlib.Path(__file__).parent, auto_mkdir=True)
CACHE = HERE / "datasets"

class VHICombiner(Combiner):
    """Default combiner.

    Delegates transformation to preprocessors and postprocessors that are passed in at initialization time and are, in
    turn, passed into Kerchunk's MultiZarrToZarr implementation.

    Parameters
    ----------
    output_folder : FileSpec
        Location of folder to write combined Zarr JSON to. The filename will be generated dynamically to be unique.

    concat_dims : list[str]
        Names of the dimensions to expand with

    identical_dims : list[str]
        Variables that are to be copied across from the first input dataset, because they do not vary

    preprocessors : list[Preprocessor]
        Preprocessors to apply when combining data. Preproccessors are callables of the same kind used by
        `kerchunk.MultiZarrToZarr`. What little documentation there is of them seems to be here:
        https://fsspec.github.io/kerchunk/tutorial.html#preprocessing


    postprocessors : list[Postprocessor]
        Postprocessors to apply when combining data. Postproccessors are callables of the same kind used by
        `kerchunk.MultiZarrToZarr`. What little documentation there is of them seems to be here:
        https://fsspec.github.io/kerchunk/tutorial.html#postprocessing
    """

    identical_dimensions = ["HEIGHT", "WIDTH"]
    """
    List of dimension(s) whose values are identical in all input datasets.
    This saves Kerchunk time by having it read these dimensions only one time, from the first input file
    """

    concat_dimensions = ["time", "sat"]
    """
    List of dimension(s) by which to concatenate input files' data variable(s)
        -- usually time, possibly with some other relevant dimension
    """

    protocol = "file"
    """
    Remote protocol string for MultiZarrToZarr and Xarray to use when opening input files.
    'File' for local, 's3' for S3, etc.
    See fssp
    """

    missing_value = -999


        #     preprocessors=[component.combine_preprocessor("fix_fill_value", -9.96921e36)],
    def __init__(
        self,
    ):
        self.output_folder = CACHE / "vhi" / "combined"
        self.concat_dims = ["time", "sat"]
        self.identical_dimensions = ["HEIGHT", "WIDTH"]
        self.protocol = "file"
        self.missing_value = -999



    @classmethod
    def mzz_opts(cls) -> dict:
        """
        Class method to populate with options to be passed to MultiZarrToZarr.
        The options dict is by default populated with class variables instantiated above;
        optional additional parameters can be added as per the needs of the input dataset

        VHI comes with no time dimension (since files are provided per time step);
        this must be created using the `coo_map` option in order to stack (concatenate) data by time in MultiZarrToZarr
        """
        coo_mapper = {"time": "attr:time_coverage_start", "sat": "attr:satellite_name"}
        opts = dict(
            remote_protocol=cls.protocol,
            identical_dims=cls.identical_dimensions,
            coo_map=coo_mapper,
            concat_dims=cls.concat_dimensions,
        )
        return opts

    @classmethod
    def preprocess_kerchunk(cls, refs: dict) -> dict:
        """
        Class method to populate with the specific preprocessing routine of each child class (if relevant),
        whilst the file is being read by Kerchunk.
        Note this function usually works by manipulating Kerchunk's internal "refs"
         -- the zarr dictionary generated by Kerchunk.

        Attributes are labeled inconsistently in VHI historical files;
        for instance, some older VHI files come without attributes used to calculate timestamps from newer files.
        These can be standardized from alternate attributes during preprocessing.


        Parameters
        ----------
        refs: dict
            A dictionary of DataSet attributes and information automatically supplied by Kerchunk

        Returns
        -------
        dict
            A dictionary of DataSet attributes and information automatically supplied by Kerchunk,
            reformatted as needed
        """
        # refs = super().preprocess_kerchunk(refs)
        ref_names = set()
        file_match_pattern = "(.*?)/"
        for ref in refs:
            if re.match(file_match_pattern, ref) is not None:
                ref_names.add(re.match(file_match_pattern, ref).group(1))
        for ref in ref_names:
            fill_value_fix = json.loads(refs[f"{ref}/.zarray"])
            fill_value_fix["fill_value"] = cls.missing_value
            refs[f"{ref}/.zarray"] = json.dumps(fill_value_fix)

        # Calculate time_coverage_start from the day of the year
        if "time_coverage_start" not in refs[".zattrs"]:
            time_cov_fix = orjson.loads(refs[".zattrs"])
            start_time_dt = datetime.datetime.strptime(
                str(time_cov_fix["YEAR"]) + "-" + str(time_cov_fix["DATE_BEGIN"]),
                "%Y-%j",
            )
            time_cov_fix["time_coverage_start"] = datetime.datetime.strftime(start_time_dt, "%Y%m%d%H%M%S0")
            refs[".zattrs"] = orjson.dumps(time_cov_fix) 

        # Prepare a satellite_name attribute where it's missing
        if "satellite_name" not in refs[".zattrs"]:
            sat_fix = orjson.loads(refs[".zattrs"])
            sat_fix["satellite_name"] = sat_fix["SATELLITE"]
            refs[".zattrs"] = orjson.dumps(sat_fix)

        return refs

    

    def __call__(self, sources: list[FileSpec]) -> xarray.Dataset:
        """Implementation of meth:`Combiner.__call__`.

        Calls `kerchunk.MultiZarrToZarr`.
        """

        def preprocessor(preprocessors):
            def preprocess(refs):
                for processor in preprocessors:
                    refs = processor(refs)

                return refs

            return preprocess

        def postprocessor(postprocessors):
            def postprocess(dataset):
                for processor in postprocessors:
                    dataset = processor(dataset)

                return dataset

            return postprocess
        coo_mapper = {"time": "attr:time_coverage_start", "sat": "attr:satellite_name"}
        ensemble = combine.MultiZarrToZarr(
            [source.path for source in sources],
            remote_protocol=self.protocol,
            identical_dims=self.identical_dimensions,
            coo_map=coo_mapper,
            concat_dims=self.concat_dimensions,
            preprocess=self.preprocess_kerchunk,
            # postprocess=postprocessor(self.postprocessors),
        )

        output = self.output_folder / f"combined_zarr_{int(time.time()):d}.json"
        with output.open("wb") as f_out:
            f_out.write(orjson.dumps(ensemble.translate()))

        return xarray.open_dataset(
            "reference://",
            engine="zarr",
            backend_kwargs={
                "consolidated": False,
                "storage_options": {
                    "fo": output.path,
                    "remote_protocol": output.fs.protocol[0],  # Does this always work for protocol?
                },
            },
        )


class CombinePreprocessor(abc.ABC):
    """See: https://fsspec.github.io/kerchunk/tutorial.html#preprocessing"""

    @abc.abstractmethod
    def __call__(self, refs: dict) -> dict:
        """See: https://fsspec.github.io/kerchunk/tutorial.html#preprocessing"""


class CombinePostprocessor(abc.ABC):
    """See: https://fsspec.github.io/kerchunk/tutorial.html#postprocessing"""

    @abc.abstractmethod
    def __call__(self, refs: dict) -> dict:
        """See: https://fsspec.github.io/kerchunk/tutorial.html#postprocessing"""