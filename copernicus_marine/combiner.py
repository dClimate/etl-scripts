
from kerchunk import combine
import abc
import time
import json
import re

import orjson
import xarray

import pathlib
import base64
import numpy as np

from .base_values import (
    CopernicusOceanSeaSurfaceHeightValues,
    CopernicusOceanTemp0p5DepthValues,
    CopernicusOceanTemp1p5DepthValues,
    CopernicusOceanTemp6p5DepthValues,
    CopernicusOceanSalinity0p5DepthValues,
    CopernicusOceanSalinity1p5DepthValues,
    CopernicusOceanSalinity2p6DepthValues,
    CopernicusOceanSalinity25DepthValues,
    CopernicusOceanSalinity109DepthValues
)

from dc_etl import filespec


from dc_etl.filespec import FileSpec
from dc_etl.combine import Combiner

HERE = filespec.file(pathlib.Path(__file__).parent, auto_mkdir=True)
CACHE = HERE / "datasets"


class CopernicusMarineCombiner(Combiner):

    """Default combiner.

    Delegates transformation to preprocessors and postprocessors that are passed in at initialization time and are, in
    turn, passed into Kerchunk's MultiZarrToZarr implementation.

    Parameters
    ----------
    output_folder : FileSpec
        Location of folder to write combined Zarr JSON to. The filename will be generated dynamically to be unique.

    concat_dims : list[str]
        Names of the dimensions to expand with

    identical_dims : list[str]
        Variables that are to be copied across from the first input dataset, because they do not vary

    preprocessors : list[Preprocessor]
        Preprocessors to apply when combining data. Preproccessors are callables of the same kind used by
        `kerchunk.MultiZarrToZarr`. What little documentation there is of them seems to be here:
        https://fsspec.github.io/kerchunk/tutorial.html#preprocessing


    postprocessors : list[Postprocessor]
        Postprocessors to apply when combining data. Postproccessors are callables of the same kind used by
        `kerchunk.MultiZarrToZarr`. What little documentation there is of them seems to be here:
        https://fsspec.github.io/kerchunk/tutorial.html#postprocessing
    """

    identical_dimensions = ["longitude"]
    """
    List of dimension(s) whose values are identical in all input datasets.
    This saves Kerchunk time by having it read these dimensions only one time, from the first input file
    """

    concat_dimensions = ["time"]
    """
    List of dimension(s) by which to concatenate input files' data variable(s)
        -- usually time, possibly with some other relevant dimension
    """

    protocol = "file"
    """
    Remote protocol string for MultiZarrToZarr and Xarray to use when opening input files.
    'File' for local, 's3' for S3, etc.
    See fssp
    """

    #     preprocessors=[component.combine_preprocessor("fix_fill_value", -9.96921e36)],
    def __init__(
        self,
        output_folder,
        concat_dims,
        identical_dims,
        # preprocessors=[component.combine_preprocessor("fix_fill_value", -9.96921e36)],
    ):
        self.output_folder = output_folder
        self.concat_dims = concat_dims
        self.identical_dimensions = identical_dims
        self.protocol = "file"

    def preprocess_kerchunk(self, refs: dict) -> dict:
        """
        Class method to populate with the specific preprocessing routine of each child class (if relevant),
        whilst the file is being read by Kerchunk.

        Note this function works by manipulating Kerchunk's internal "refs"
         -- the dictionary representing a Zarr generated by Kerchunk.

        Parameters
        ----------
        refs: dict
            A dictionary of DataSet attributes and information automatically supplied by Kerchunk

        Returns
        -------
        dict
            A dictionary of DataSet attributes and information automatically supplied by Kerchunk,
            reformatted as needed
        """
        ref_names = set()
        file_match_pattern = "(.*?)/"
        for ref in refs:
            if re.match(file_match_pattern, ref) is not None:
                ref_names.add(re.match(file_match_pattern, ref).group(1))
        for ref in ref_names:
            fill_value_fix = json.loads(refs[f"{ref}/.zarray"])
            fill_value_fix["fill_value"] = self.missing_value
            refs[f"{ref}/.zarray"] = json.dumps(fill_value_fix)

        return refs

    

    def __call__(self, sources: list[FileSpec]) -> xarray.Dataset:
        """Implementation of meth:`Combiner.__call__`.

        Calls `kerchunk.MultiZarrToZarr`.
        """
        def preprocessor(preprocessors):
            def preprocess(refs):
                for processor in preprocessors:
                    refs = processor(refs)

                return refs

            return preprocess

        def postprocessor(postprocessors):
            def postprocess(dataset):
                for processor in postprocessors:
                    dataset = processor(dataset)

                return dataset

            return postprocess
        ensemble = combine.MultiZarrToZarr(
            [source.path for source in sources],
            remote_protocol=self.protocol,
            identical_dims=self.identical_dimensions,
            concat_dims=self.concat_dimensions,
            preprocess=self.preprocess_kerchunk,
            # postprocess=postprocessor(self.postprocessors),
        )

        output = self.output_folder / f"combined_zarr_{int(time.time()):d}.json"
        with output.open("wb") as f_out:
            f_out.write(orjson.dumps(ensemble.translate()))

        return xarray.open_dataset(
            "reference://",
            engine="zarr",
            backend_kwargs={
                "consolidated": False,
                "storage_options": {
                    "fo": output.path,
                    "remote_protocol": output.fs.protocol[0],  # Does this always work for protocol?
                },
            },
        )




class GlobalPhysicsCombiner(CopernicusMarineCombiner):
    """Global Physics Combiner.

    """

    # Reimplementing the default combiner
    def preprocess_kerchunk(self, refs: dict) -> dict:
        """
        Class method to populate with the specific preprocessing routine of each child class (if relevant),
        whilst the file is being read by Kerchunk.

        Note this function works by manipulating Kerchunk's internal "refs"
         -- the dictionary representing a Zarr generated by Kerchunk.

        Parameters
        ----------
        refs: dict
            A dictionary of DataSet attributes and information automatically supplied by Kerchunk

        Returns
        -------
        dict
            A dictionary of DataSet attributes and information automatically supplied by Kerchunk,
            reformatted as needed
        """
        refs = {key: value for key, value in refs.items() if "depth" not in key}

        ref_names = set()
        file_match_pattern = "(.*?)/"
        for ref in refs:
            if re.match(file_match_pattern, ref) is not None:
                ref_names.add(re.match(file_match_pattern, ref).group(1))
        for ref in ref_names:
            fill_value_fix = json.loads(refs[f"{ref}/.zarray"])
            fill_value_fix["fill_value"] = self.missing_value
            refs[f"{ref}/.zarray"] = json.dumps(fill_value_fix)
    
        # set consistent coordinates
        for ref, new_arr_coords, index in [
            ("latitude", (-80, 90, 2041), slice(None)),
            ("longitude", (-180, 180, 4321), slice(0, -1)),
        ]:
            coords_arr = np.linspace(*new_arr_coords, dtype=np.float32)[index]  # need to remove last longitude value
            coords_bytes = coords_arr.tobytes()
            refs[f"{ref}/0"] = "base64:" + base64.b64encode(coords_bytes).decode()

        # fix dtype
        dtype_arr = json.loads(refs[f"{self.data_var}/.zarray"])
        dtype_arr["dtype"] = "<f4"
        refs[f"{self.data_var}/.zarray"] = json.dumps(dtype_arr)

        # remove adjustment params
        attrs_arr = json.loads(refs[f"{self.data_var}/.zarray"])
        for prop in ["scale_factor", "add_offset"]:
            attrs_arr.pop(prop, None)
        refs[f"{self.data_var}/.zarray"] = json.dumps(attrs_arr)

        return refs


class CombinePreprocessor(abc.ABC):
    """See: https://fsspec.github.io/kerchunk/tutorial.html#preprocessing"""

    @abc.abstractmethod
    def __call__(self, refs: dict) -> dict:
        """See: https://fsspec.github.io/kerchunk/tutorial.html#preprocessing"""


class CombinePostprocessor(abc.ABC):
    """See: https://fsspec.github.io/kerchunk/tutorial.html#postprocessing"""

    @abc.abstractmethod
    def __call__(self, refs: dict) -> dict:
        """See: https://fsspec.github.io/kerchunk/tutorial.html#postprocessing"""


class CopernicusOceanSeaSurfaceHeightCombiner(CopernicusMarineCombiner, CopernicusOceanSeaSurfaceHeightValues):
    def __init__(self, output_folder, concat_dims, identical_dims):
        CopernicusOceanSeaSurfaceHeightValues.__init__(self)
        CopernicusMarineCombiner.__init__(self, output_folder=output_folder, concat_dims=concat_dims, identical_dims=identical_dims)


class CopernicusOceanTemp0p5DepthCombiner(GlobalPhysicsCombiner, CopernicusOceanTemp0p5DepthValues):

    def __init__(self, output_folder, concat_dims, identical_dims):
        CopernicusOceanTemp0p5DepthValues.__init__(self)
        GlobalPhysicsCombiner.__init__(self, output_folder=output_folder, concat_dims=concat_dims, identical_dims=identical_dims)

class CopernicusOceanTemp1p5DepthCombiner(GlobalPhysicsCombiner, CopernicusOceanTemp1p5DepthValues):

    def __init__(self, output_folder, concat_dims, identical_dims):
        CopernicusOceanTemp1p5DepthValues.__init__(self)
        GlobalPhysicsCombiner.__init__(self, output_folder=output_folder, concat_dims=concat_dims, identical_dims=identical_dims)

class CopernicusOceanTemp6p5DepthCombiner(GlobalPhysicsCombiner, CopernicusOceanTemp6p5DepthValues):

    def __init__(self, output_folder, concat_dims, identical_dims):
        CopernicusOceanTemp6p5DepthValues.__init__(self)
        GlobalPhysicsCombiner.__init__(self, output_folder=output_folder, concat_dims=concat_dims, identical_dims=identical_dims)

class CopernicusOceanSalinity0p5DepthCombiner(GlobalPhysicsCombiner, CopernicusOceanSalinity0p5DepthValues):

    def __init__(self, output_folder, concat_dims, identical_dims):
        CopernicusOceanSalinity0p5DepthValues.__init__(self)
        GlobalPhysicsCombiner.__init__(self, output_folder=output_folder, concat_dims=concat_dims, identical_dims=identical_dims)

class CopernicusOceanSalinity1p5DepthCombiner(GlobalPhysicsCombiner, CopernicusOceanSalinity1p5DepthValues):

    def __init__(self, output_folder, concat_dims, identical_dims):
        CopernicusOceanSalinity1p5DepthValues.__init__(self)
        GlobalPhysicsCombiner.__init__(self, output_folder=output_folder, concat_dims=concat_dims, identical_dims=identical_dims)
    
class CopernicusOceanSalinity2p6DepthCombiner(GlobalPhysicsCombiner, CopernicusOceanSalinity2p6DepthValues):

    def __init__(self, output_folder, concat_dims, identical_dims):
        CopernicusOceanSalinity2p6DepthValues.__init__(self)
        GlobalPhysicsCombiner.__init__(self, output_folder=output_folder, concat_dims=concat_dims, identical_dims=identical_dims)

class CopernicusOceanSalinity25DepthCombiner(GlobalPhysicsCombiner, CopernicusOceanSalinity25DepthValues):
    
    def __init__(self, output_folder, concat_dims, identical_dims):
        CopernicusOceanSalinity25DepthValues.__init__(self)
        GlobalPhysicsCombiner.__init__(self, output_folder=output_folder, concat_dims=concat_dims, identical_dims=identical_dims)

class CopernicusOceanSalinity109DepthCombiner(GlobalPhysicsCombiner, CopernicusOceanSalinity109DepthValues):
   
    def __init__(self, output_folder, concat_dims, identical_dims):
        CopernicusOceanSalinity109DepthValues.__init__(self)
        GlobalPhysicsCombiner.__init__(self, output_folder=output_folder, concat_dims=concat_dims, identical_dims=identical_dims)